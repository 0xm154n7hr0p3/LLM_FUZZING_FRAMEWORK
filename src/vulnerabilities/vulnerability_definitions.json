
{
    "system_prompt_leakage": {
      "type": "Fuite du Prompt Système",
      "description": "Les LLM peuvent parfois révéler le prompt système (invisible à l’utilisateur) qui contient des instructions internes ou de configuration.",
      "impact": "Les attaquants peuvent comprendre et exploiter le fonctionnement interne du modèle, ou contourner les mesures de sécurité.",
      "remediation": "- Cacher strictement les prompts système.
- Concevoir des prompts résistants aux fuites.
- Ajouter des contrôles d'intégrité sur les réponses."
    },
    "unbounded_consumption": {
      "type": "Consommation Illimitée",
      "description": "Les LLM peuvent consommer des ressources de manière excessive (temps de calcul, mémoire, requêtes API), surtout en cas de prompt mal formé ou volumineux.",
      "impact": "Déni de service (DoS), augmentation des coûts, ou interruption de service pour d'autres utilisateurs.",
      "remediation": "- Limiter la taille des prompts et des sorties.
- Mettre en place un quota ou système de facturation.
- Surveiller les performances et les pics de consommation."
    },
    "misinformation": {
      "type": "Désinformation",
      "description": "Misinformation vulnerabilities occur when an LLM can be manipulated into generating false, misleading, or fabricated information that is presented as factual. This includes hallucinating references, citations, or creating convincing but false content.",
      "impact": "Users may make decisions based on incorrect information, potentially causing harm in contexts like health, finance, or safety. It can damage trust in the system and potentially create legal liability for providers of the LLM service.",
      "remediation": "Train models to acknowledge uncertainty, implement fact-checking mechanisms for sensitive domains, provide sources and confidence levels with responses, add clarifications when information might be outdated, and design systems to refuse to make definitive claims in high-risk areas without verification."
    },
    "sensitive_information_disclosure": {
      "type": "Sensitive Information Disclosure",
      "description": "This vulnerability occurs when an LLM reveals or fails to properly protect personally identifiable information (PII), confidential data, or other sensitive information that should remain private.",
      "impact": "Can lead to privacy violations, breach of confidentiality, regulatory non-compliance, and potential harm to individuals whose information is exposed. May result in legal consequences and damage to trust.",
      "remediation": "Implement robust data sanitization before processing, train models to recognize and refuse to disclose sensitive information, incorporate PII detection tools, create strong guardrails for handling potential PII in prompts and responses, and regularly audit system behavior with sensitive data."
    },
    "prompt_injection": {
      "type": "Prompt Injection",
      "description": "Prompt injection occurs when malicious inputs are crafted to override or manipulate the LLM's original instructions, causing it to ignore safety constraints or perform unintended actions.",
      "impact": "Can bypass security measures, lead to harmful outputs, ignore content policies, and potentially access restricted functionality or information. Undermines the integrity of the system's designed behavior.",
      "remediation": "Implement input validation, use robust prompt engineering techniques that resist injection, separate and validate different parts of the prompt, establish strong instruction boundaries, and conduct regular security testing with a variety of injection techniques."
    }
  }