{
  "system_prompt_leakage": {
    "type": "Fuite du Prompt Système",
    "description": "Les LLM peuvent parfois révéler le prompt système (invisible à l’utilisateur) qui contient des instructions internes ou de configuration.",
    "impact": "Les attaquants peuvent comprendre et exploiter le fonctionnement interne du modèle, ou contourner les mesures de sécurité.",
    "remediation": [
      "Cacher strictement les prompts système.",
      "Concevoir des prompts résistants aux fuites.",
      "Ajouter des contrôles d'intégrité sur les réponses."
    ]
  },
  "unbounded_consumption": {
    "type": "Consommation Illimitée",
    "description": "Les LLM peuvent consommer des ressources de manière excessive (temps de calcul, mémoire, requêtes API), surtout en cas de prompt mal formé ou volumineux.",
    "impact": "Déni de service (DoS), augmentation des coûts, ou interruption de service pour d'autres utilisateurs.",
    "remediation": [
      "Limiter la taille des prompts et des sorties.",
      "Mettre en place un quota ou système de facturation.",
      "Surveiller les performances et les pics de consommation."
    ]
  },
  "misinformation": {
    "type": "Désinformation",
    "description": "Le LLM peut générer des contenus factuellement incorrects ou inventés, souvent de manière convaincante.",
    "impact": "Risque de diffusion massive de fausses informations, atteinte à la réputation, ou mauvaise prise de décision.",
    "remediation": [
      "Mettre en place une vérification croisée avec des sources fiables.",
      "Ajouter des avertissements sur les incertitudes.",
      "Utiliser des modèles spécialisés pour la véracité factuelle."
    ]
  },
  "sensitive_information_disclosure": {
    "type": "Divulgation d’Informations Sensibles",
    "description": "Un LLM peut involontairement divulguer des informations sensibles, comme des données personnelles ou confidentielles, qu’il a vues pendant son entraînement.",
    "impact": "Risque de fuite de secrets professionnels, de données personnelles, ou d'informations réglementées (ex : RGPD, HIPAA).",
    "remediation": [
      "Surveiller et restreindre les données utilisées lors de l'entraînement.",
      "Mettre en place des filtres post-réponse.",
      "Tester le modèle pour les fuites potentielles."
    ]
  },
  "prompt_injection": {
    "type": "Prompt Injection",
    "description": "Les injections de prompt se produisent lorsque des entrées malveillantes sont introduites dans les requêtes envoyées à un LLM. Cela peut manipuler ou détourner ses réponses.",
    "impact": "Le modèle peut exécuter des actions non prévues, contourner les contrôles de sécurité, ou divulguer des informations sensibles.",
    "remediation": [
      "Séparer clairement les instructions système des entrées utilisateur.",
      "Filtrer et valider les prompts entrants.",
      "Utiliser des modèles à permissions restreintes."
    ]
  },
  "supply_chain_vulnerabilities": {
    "type": "Vulnérabilités de la Chaîne d’Approvisionnement",
    "description": "Les LLM dépendent de nombreuses bibliothèques tierces, ensembles de données, et modèles externes. Si ces éléments sont compromis, cela peut introduire des vulnérabilités ou du code malveillant.",
    "impact": "Un attaquant peut compromettre la sécurité du modèle en injectant des données ou composants malveillants, pouvant mener à des fuites, manipulations ou compromissions à grande échelle.",
    "remediation": [
      "Vérifier l’intégrité et la provenance des dépendances utilisées.",
      "Mettre à jour régulièrement les bibliothèques et composants.",
      "Auditer les sources de données et les modèles tiers."
    ]
  },
  "training_data_poisoning": {
    "type": "Empoisonnement des Données d’Entraînement",
    "description": "Des données malveillantes ou biaisées peuvent être intentionnellement intégrées dans le corpus d’entraînement d’un LLM afin de manipuler son comportement ou ses réponses.",
    "impact": "Cela peut provoquer des réponses incorrectes, introduire des biais, ou servir de vecteur de manipulation pour des attaques futures par des acteurs malveillants.",
    "remediation": [
      "Contrôler strictement les sources de données utilisées pour l'entraînement.",
      "Détecter et éliminer les données suspectes avant l’intégration.",
      "Utiliser des techniques de robustesse et d’audit des jeux de données."
    ]
  },
  "insecure_output_handling": {
    "type": "Gestion Insecure des Sorties",
    "description": "Les réponses générées par un LLM peuvent être utilisées sans validation dans des systèmes en aval, ce qui peut introduire des vulnérabilités comme l’injection de code ou des attaques côté client.",
    "impact": "Cela peut mener à l'exécution de code malveillant, à des attaques XSS, ou à la compromission d'applications qui traitent automatiquement les réponses du LLM.",
    "remediation": [
      "Valider et filtrer les sorties du LLM avant toute utilisation.",
      "Ne jamais exécuter directement les données générées.",
      "Utiliser des mécanismes de désinfection pour les contenus dynamiques."
    ]
  },
  "excessive_agency": {
    "type": "Agence Excessive",
    "description": "Un LLM peut être relié à des outils externes (API, scripts, automations) avec peu ou pas de contrôle, ce qui lui donne un pouvoir excessif pour agir dans le système ou l’environnement réel.",
    "impact": "Un attaquant pourrait exploiter cette connexion pour réaliser des actions dangereuses, comme envoyer des e-mails, exécuter du code, ou modifier des systèmes de production.",
    "remediation": [
      "Limiter les capacités du LLM à interagir avec des systèmes critiques.",
      "Implémenter une couche de validation humaine ou de contrôle d'accès.",
      "Suivre les actions du LLM avec des logs détaillés."
    ]
  },
  "vector_embedding_weaknesses": {
    "type": "Failles dans les Vecteurs et les Représentations",
    "description": "Les systèmes qui utilisent des vecteurs d’embedding (par ex. pour la recherche sémantique ou la mémoire) peuvent être attaqués via l’injection de contenus similaires ou malicieux dans l’espace vectoriel.",
    "impact": "Cela peut biaiser les résultats de recherche, causer des collisions de contenu, ou servir de porte d’entrée pour manipuler les réponses du modèle.",
    "remediation": [
      "Surveiller et valider les entrées utilisées dans les systèmes vectoriels.",
      "Détecter les anomalies dans l’espace d’embedding.",
      "Utiliser des techniques de séparation sémantique et de filtrage des vecteurs."
    ]
  }
}
